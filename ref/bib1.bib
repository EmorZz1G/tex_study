% Encoding: UTF-8

@Article{Mnih2013,
  author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  title         = {Playing Atari with Deep Reinforcement Learning},
  year          = {2013},
  month         = dec,
  abstract      = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  eprint        = {1312.5602},
  file          = {:http\://arxiv.org/pdf/1312.5602v1:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Ong2015,
  author        = {Hao Yi Ong and Kevin Chavez and Augustus Hong},
  title         = {Distributed Deep Q-Learning},
  year          = {2015},
  month         = aug,
  abstract      = {We propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framework to the context of efficiently training reinforcement learning agents. As a result, the method is completely asynchronous and scales well with the number of machines. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to achieve reasonable success on a simple game with minimal parameter tuning.},
  archiveprefix = {arXiv},
  eprint        = {1508.04186},
  file          = {:http\://arxiv.org/pdf/1508.04186v2:PDF},
  keywords      = {cs.LG, cs.AI, cs.DC, cs.NE},
  primaryclass  = {cs.LG},
}

@Article{Schaul2015,
  author        = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  title         = {Prioritized Experience Replay},
  year          = {2015},
  month         = nov,
  abstract      = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archiveprefix = {arXiv},
  eprint        = {1511.05952},
  file          = {:http\://arxiv.org/pdf/1511.05952v4:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Hasselt2015,
  author        = {Hado van Hasselt and Arthur Guez and David Silver},
  title         = {Deep Reinforcement Learning with Double Q-learning},
  year          = {2015},
  month         = sep,
  abstract      = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archiveprefix = {arXiv},
  eprint        = {1509.06461},
  file          = {:Hasselt2015 - Deep Reinforcement Learning with Double Q Learning.pdf:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectoryLatex-Emor-LAPTOP-UABOPOLQ:C:\\Users\\Emor\\AppData\\Roaming\\MiKTeX\\bibtex;}
